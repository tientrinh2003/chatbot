#!/usr/bin/env python3
"""
Enhanced SmartBP Chatbot API with SBM Integration
Supports role-based conversations with patient data context
"""

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import os
import logging
import warnings
import shutil
import uuid
import time
import json
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any, Literal
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate

# Suppress noisy pypdf warnings
logging.getLogger("pypdf._reader").setLevel(logging.ERROR)

# Note: Memory and chains moved in LangChain 1.x
# We'll use simpler LLM + retriever flow instead of ConversationalRetrievalChain

# Import Ollama components (required)
from langchain_ollama import OllamaEmbeddings, ChatOllama
import ollama

# Import HuggingFace embeddings for RAG
from langchain_huggingface import HuggingFaceEmbeddings
import sentence_transformers
from langdetect import detect, LangDetectException
from dotenv import load_dotenv
from fastapi.responses import JSONResponse

# Load environment variables
load_dotenv()

app = FastAPI(title="SmartBP Chatbot API", version="2.0.0")

# CORS configuration
cors_origins = os.getenv("CORS_ORIGINS", "http://localhost:3000,http://127.0.0.1:3000").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global variables for RAG components
vectorstore = None
conversation_chains = {}
embeddings_model = None  # Cache embeddings to reuse

# Lifespan context manager (FastAPI 0.93+)
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan handler for startup and shutdown"""
    # Startup
    logging.info("üöÄ Starting SmartBP Chatbot API...")
    logging.info("üìö Initializing knowledge base...")
    
    # Delete old incompatible vectorstore if exists
    old_db_path = "./db"
    if os.path.exists(old_db_path) and os.path.isdir(old_db_path):
        try:
            # Check if it's the old 768-dim vectorstore
            chroma_meta = os.path.join(old_db_path, "chroma.sqlite3")
            if os.path.exists(chroma_meta):
                logging.info(f"üóëÔ∏è  Removing old incompatible vectorstore: {old_db_path}")
                shutil.rmtree(old_db_path)
        except Exception as e:
            logging.warning(f"Could not remove old db: {e}")
    
    # Initialize once
    initialize_vectorstore()
    logging.info("‚úÖ Startup completed")
    
    yield  # App runs here
    
    # Shutdown
    logging.info("üõë Shutting down...")
    conversation_chains.clear()
    
app = FastAPI(
    title="SmartBP Chatbot API", 
    version="2.0.0",
    lifespan=lifespan
)

# Remove old on_event decorators
# @app.on_event("startup")  # DELETED
# @app.on_event("shutdown")  # DELETED

DATA_PATH = os.getenv("DATA_DIR", "./data")
MODEL_NAME = os.getenv("LLM_MODEL", "llama3.1:8b")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
# Use consistent 384-dim embeddings to match vectorstore
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
DB_PATH = os.getenv("PERSIST_DIR", "./db/chroma_db_384")
EMBEDDING_DIM = 384  # MiniLM produces 384-dim embeddings

# Fallback configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
USE_MOCK_RESPONSES = os.getenv("USE_MOCK_RESPONSES", "false").lower() == "true"

# Pydantic Models
class UserContext(BaseModel):
    id: str
    name: Optional[str] = None
    email: Optional[str] = None
    role: Literal["PATIENT", "DOCTOR", "ADMIN"]
    dateOfBirth: Optional[str] = None
    gender: Optional[Literal["MALE", "FEMALE", "OTHER"]] = None
    phone: Optional[str] = None

class MeasurementData(BaseModel):
    id: str
    sys: int
    dia: int
    pulse: int
    method: Literal["BLUETOOTH", "MANUAL"]
    takenAt: str
    trend: Optional[Dict[str, Any]] = None

class PatientSummary(BaseModel):
    latest_measurements: List[MeasurementData] = []
    measurement_count: int = 0
    avg_sys: float = 0
    avg_dia: float = 0
    risk_assessment: str = "Unknown"
    recent_notes: Optional[List[str]] = []

class DoctorContext(BaseModel):
    assigned_patients_count: int = 0
    recent_alerts: Optional[List[str]] = []
    pending_reviews: Optional[int] = 0

class SessionMetadata(BaseModel):
    device_info: Optional[str] = None
    location: Optional[str] = None

class ChatContext(BaseModel):
    user: UserContext
    role_specific_data: Optional[Dict[str, Any]] = None
    timestamp: str
    session_metadata: Optional[SessionMetadata] = None

class EnhancedChatRequest(BaseModel):
    message: str
    user_id: str
    conversation_id: Optional[str] = None
    context: ChatContext
    language: Optional[Literal["en", "vi", "auto"]] = "auto"  # New language parameter

class DataInsights(BaseModel):
    mentioned_measurements: bool = False
    health_recommendations: List[str] = []
    follow_up_actions: List[str] = []

class EnhancedChatResponse(BaseModel):
    success: bool
    response: str
    conversation_id: str
    suggestions: Optional[List[str]] = []
    requires_medical_attention: bool = False
    data_insights: Optional[DataInsights] = None
    detected_language: Optional[str] = None  # New field for detected language

# Language detection function
def detect_language(text: str) -> str:
    """Detect language from input text"""
    try:
        detected = detect(text)
        return "en" if detected == "en" else "vi"
    except LangDetectException:
        # Default to Vietnamese if detection fails
        return "vi"

# Unified intelligent prompt template that adapts to user's language
UNIFIED_TEMPLATE = """You are SmartBP, an intelligent health assistant specializing in blood pressure management and healthcare.

CRITICAL LANGUAGE RULE: 
- If the user asks in Vietnamese (contains Vietnamese characters like √°, √†, ·∫°, ·ªÉ, ·ªü, ·ªß, etc.), respond COMPLETELY in Vietnamese
- If the user asks in English, respond COMPLETELY in English  
- NEVER mix languages in a single response
- When in doubt, analyze the user's question for Vietnamese words like "huy·∫øt √°p", "s·ª©c kh·ªèe", "b√°c sƒ©" and respond in Vietnamese

USER CONTEXT:
- Name: {patient_name}
- Age: {patient_age} 
- Role: {role}
- Recent BP average: {avg_bp} mmHg
- Total measurements: {measurement_count}
- Risk level: {risk_level}
- Latest readings: {recent_measurements}

CORE GUIDELINES:
- ALWAYS match the user's language naturally
- Use their actual health data when relevant
- For serious symptoms, urgently recommend seeing a doctor
- Provide personalized advice based on their BP trends
- Be supportive and encouraging
- Draw from medical knowledge base when needed

Medical Knowledge Context: {context}
Chat History: {chat_history}

User's Question: {question}

Response (in user's language):"""

# Global variables for RAG components
vectorstore = None
conversation_chains = {}

def load_all_documents():
    """Load all document files from data directory"""
    documents = []
    data_dir = "data"
    
    if not os.path.exists(data_dir):
        logging.warning(f"Data directory '{data_dir}' not found")
        return documents
    
    for filename in os.listdir(data_dir):
        file_path = os.path.join(data_dir, filename)
        
        try:
            if filename.endswith('.txt'):
                loader = TextLoader(file_path, encoding='utf-8')
                docs = loader.load()
                # Add metadata
                for doc in docs:
                    doc.metadata['source'] = filename
                    doc.metadata['type'] = 'text'
                documents.extend(docs)
                logging.info(f"üìÑ Loaded text file: {filename}")
                
            elif filename.endswith('.pdf'):
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                # Add metadata  
                for doc in docs:
                    doc.metadata['source'] = filename
                    doc.metadata['type'] = 'pdf'
                documents.extend(docs)
                logging.info(f"üìÑ Loaded PDF file: {filename}")
                
        except Exception as e:
            logging.error(f"‚ùå Error loading {filename}: {e}")
            continue
    
    logging.info(f"üìö Total documents loaded: {len(documents)}")
    return documents

def initialize_vectorstore():
    """Initialize vectorstore with all documents - only called once at startup"""
    global vectorstore, embeddings_model
    
    # Early return if already initialized
    if vectorstore is not None:
        logging.info("‚úÖ Vectorstore already initialized, skipping...")
        return vectorstore
    
    try:
        # Load all documents
        documents = load_all_documents()
        
        if not documents:
            logging.warning("‚ö†Ô∏è No documents loaded, using fallback responses only")
            return None
        
        # Split documents into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        splits = text_splitter.split_documents(documents)
        logging.info(f"üìÑ Document chunks created: {len(splits)}")
        
        # Initialize embeddings - use 384-dim MiniLM for consistency
        embeddings_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'}
        )
        # Verify dimension
        test_embed = embeddings_model.embed_query("test")
        actual_dim = len(test_embed)
        logging.info(f"‚úÖ Using HuggingFace embeddings - Dimension: {actual_dim}")
        
        if actual_dim != EMBEDDING_DIM:
            logging.warning(f"‚ö†Ô∏è Embedding dimension mismatch: expected {EMBEDDING_DIM}, got {actual_dim}")
        
        # Create or load vectorstore
        try:
            vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=embeddings_model,
                persist_directory=DB_PATH
            )
            logging.info(f"‚úÖ Vectorstore initialized successfully at {DB_PATH}")
        except Exception as e:
            logging.error(f"‚ùå Vectorstore creation failed: {e}")
            # Try with fallback
            logging.info("üí° Attempting fallback...")
            vectorstore = None
        
        return vectorstore
        
    except Exception as e:
        logging.error(f"‚ùå Failed to initialize vectorstore: {e}")
        import traceback
        traceback.print_exc()
        return None

class MockEmbeddings:
    """Mock embeddings for when no embedding service is available"""
    
    def embed_documents(self, texts):
        """Return mock embeddings for documents - 384-dim to match MiniLM"""
        return [[0.1] * EMBEDDING_DIM for _ in texts]
    
    def embed_query(self, text):
        """Return mock embedding for query - 384-dim to match MiniLM"""
        return [0.1] * EMBEDDING_DIM

def initialize_rag_system():
    """Initialize the RAG system with medical documents"""
    global vectorstore
    
    embeddings = None
    
    # Try different embedding services in priority order
    try:
        # 1. Try HuggingFace/SentenceTransformers (best for Vietnamese)
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                    model_kwargs={'device': 'cpu'}  # Ensure CPU usage for stability
                )
                logging.info("‚úÖ Using HuggingFace embeddings (best for Vietnamese)")
            except Exception as e:
                logging.warning(f"HuggingFace embeddings failed: {e}")
        
        # 2. Try Ollama embeddings
        if not embeddings and OLLAMA_AVAILABLE:
            try:
                ollama_client = ollama.Client()
                ollama_client.list()  # Test connection
                embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
                logging.info("‚úÖ Using Ollama embeddings")
            except Exception as e:
                logging.warning(f"Ollama embeddings failed: {e}")
        
        # 3. Try OpenAI embeddings
        if not embeddings and OPENAI_AVAILABLE and OPENAI_API_KEY:
            try:
                embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
                logging.info("‚úÖ Using OpenAI embeddings")
            except Exception as e:
                logging.warning(f"OpenAI embeddings failed: {e}")
        
        # 4. Final fallback to mock embeddings
        if not embeddings:
            embeddings = MockEmbeddings()
            logging.info("‚úÖ Using mock embeddings (fallback mode - limited RAG)")
            
    except Exception as e:
        embeddings = MockEmbeddings()
        logging.error(f"‚ùå All embedding services failed, using mock: {e}")
    
    try:
        # Load and process documents
        documents = []
        if os.path.exists(DATA_PATH):
            for filename in os.listdir(DATA_PATH):
                file_path = os.path.join(DATA_PATH, filename)
                try:
                    if filename.endswith('.txt'):
                        loader = TextLoader(file_path, encoding='utf-8')
                        documents.extend(loader.load())
                        logging.info(f"üìÑ Loaded {filename}")
                    elif filename.endswith('.pdf'):
                        loader = PyPDFLoader(file_path)
                        documents.extend(loader.load())
                        logging.info(f"üìÑ Loaded {filename}")
                except Exception as e:
                    logging.warning(f"Failed to load {filename}: {e}")
        else:
            logging.warning(f"Data directory {DATA_PATH} not found")
        
        if not documents:
            logging.warning("No documents loaded - RAG will use fallback context")
            
        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents) if documents else []
        
        # Create vector store (only if we have real embeddings and documents)
        if texts and not isinstance(embeddings, MockEmbeddings):
            try:
                vectorstore = Chroma.from_documents(
                    documents=texts,
                    embedding=embeddings,
                    persist_directory=DB_PATH
                )
                logging.info(f"‚úÖ RAG system initialized with {len(texts)} document chunks")
            except Exception as e:
                logging.warning(f"Vector store creation failed: {e} - using fallback")
                vectorstore = None
        else:
            logging.info("üìö RAG system initialized in fallback mode")
            vectorstore = None
        
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Failed to initialize RAG system: {e}")
        return False

def get_fallback_medical_context(question: str) -> str:
    """Provide medical context when RAG system is not available"""
    question_lower = question.lower()
    
    if any(word in question_lower for word in ["huy·∫øt √°p", "blood pressure", "bp", "cao", "th·∫•p"]):
        return """
Th√¥ng tin c∆° b·∫£n v·ªÅ huy·∫øt √°p:
- Huy·∫øt √°p b√¨nh th∆∞·ªùng: < 120/80 mmHg
- Ti·ªÅn tƒÉng huy·∫øt √°p: 120-129/<80 mmHg  
- TƒÉng huy·∫øt √°p ƒë·ªô 1: 130-139/80-89 mmHg
- TƒÉng huy·∫øt √°p ƒë·ªô 2: ‚â•140/90 mmHg
- Kh·ªßng ho·∫£ng tƒÉng huy·∫øt √°p: ‚â•180/120 mmHg

Y·∫øu t·ªë nguy c∆°: tu·ªïi t√°c, di truy·ªÅn, b√©o ph√¨, thi·∫øu v·∫≠n ƒë·ªông, stress, ƒÉn m·∫∑n.
Bi·∫øn ch·ª©ng: ƒë·ªôt qu·ªµ, nh·ªìi m√°u c∆° tim, suy tim, suy th·∫≠n.
Theo d√µi: ƒëo huy·∫øt √°p th∆∞·ªùng xuy√™n, u·ªëng thu·ªëc ƒë·ªÅu ƒë·∫∑n.
"""
    
    elif any(word in question_lower for word in ["ƒÉn", "diet", "dinh d∆∞·ª°ng", "th·ª©c ƒÉn", "mu·ªëi"]):
        return """
Ch·∫ø ƒë·ªô ƒÉn cho ng∆∞·ªùi tƒÉng huy·∫øt √°p (ch·∫ø ƒë·ªô DASH):
- Gi·∫£m mu·ªëi: <2.3g natri/ng√†y (1 th√¨a c√† ph√™)
- TƒÉng rau xanh, tr√°i c√¢y: 4-5 ph·∫ßn/ng√†y
- Ch·ªçn ng≈© c·ªëc nguy√™n h·∫°t
- Protein t·ª´ c√°, g√†, ƒë·∫≠u, h·∫°t
- S·ªØa √≠t b√©o: 2-3 ly/ng√†y
- H·∫°n ch·∫ø: ƒë·ªì chi√™n, ƒë·ªì ng·ªçt, ƒë·ªì ƒë√≥ng h·ªôp
- U·ªëng ƒë·ªß n∆∞·ªõc: 1.5-2L/ng√†y
"""
    
    elif any(word in question_lower for word in ["t·∫≠p", "exercise", "v·∫≠n ƒë·ªông", "th·ªÉ d·ª•c"]):
        return """
V·∫≠n ƒë·ªông cho ng∆∞·ªùi tƒÉng huy·∫øt √°p:
- Aerobic: 150 ph√∫t/tu·∫ßn c∆∞·ªùng ƒë·ªô v·ª´a (ƒëi b·ªô nhanh, b∆°i l·ªôi, ƒë·∫°p xe)
- T·∫≠p t·∫°: 2-3 l·∫ßn/tu·∫ßn, 8-12 ƒë·ªông t√°c, m·ªói ƒë·ªông t√°c 8-12 l·∫ßn
- Kh·ªüi ƒë·ªông: 5-10 ph√∫t
- Th∆∞ gi√£n: 5-10 ph√∫t
- Tr√°nh: v·∫≠n ƒë·ªông qu√° s·ª©c, nh·ªãn th·ªü khi t·∫≠p t·∫°
- Theo d√µi: ƒëo huy·∫øt √°p tr∆∞·ªõc v√† sau t·∫≠p
"""
    
    elif any(word in question_lower for word in ["thu·ªëc", "medication", "ƒëi·ªÅu tr·ªã"]):
        return """
ƒêi·ªÅu tr·ªã tƒÉng huy·∫øt √°p:
- Thu·ªëc ch·∫πn ACE: lisinopril, enalapril
- Thu·ªëc ch·∫πn th·ª• th·ªÉ angiotensin: losartan, valsartan
- Thu·ªëc l·ª£i ti·ªÉu: hydrochlorothiazide
- Thu·ªëc ch·∫πn k√™nh canxi: amlodipine, nifedipine
- Thu·ªëc ch·∫πn beta: metoprolol, atenolol

L∆∞u √Ω: U·ªëng thu·ªëc ƒë√∫ng gi·ªù, kh√¥ng t·ª± √Ω ng·ª´ng thu·ªëc, theo d√µi t√°c d·ª•ng ph·ª•.
"""
    
    return "Ki·∫øn th·ª©c y t·∫ø c∆° b·∫£n v·ªÅ qu·∫£n l√Ω s·ª©c kh·ªèe v√† tƒÉng huy·∫øt √°p."

def generate_enhanced_mock_response(message: str, role: str, language: str, medical_context: str, context: 'ChatContext') -> str:
    """Generate enhanced mock responses with user context and medical knowledge"""
    message_lower = message.lower()
    user_name = context.user.name or "b·∫°n"
    
    # Enhanced language detection
    if language == "auto":
        language = detect_language(message)
    
    # Improved Vietnamese detection with more comprehensive rules
    vietnamese_chars = any(char in message for char in '√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë')
    
    # Expanded Vietnamese keywords
    vietnamese_keywords = ['huy·∫øt', '√°p', 'c·ªßa', 't√¥i', 'nh∆∞', 'th·∫ø', 'n√†o', 'g√¨', 'l√†m', 'sao', 'c√°ch', 'x·ª≠', 'd·ª•ng', '·ª©ng', 'dng', 'b√°c', 'sƒ©', 'b·ªánh', 'nh√¢n', 'l√†', 'v√†', 'c√≥', 'ƒë∆∞·ª£c', 'kh√¥ng', 'ch√†o', 'xin', 'v·ªÅ', 's·ª©c', 'kh·ªèe', 'ƒëo', 'k·∫øt', 'qu·∫£']
    vietnamese_words = sum(1 for word in vietnamese_keywords if word in message_lower)
    
    # English keywords  
    english_keywords = ['blood', 'pressure', 'how', 'what', 'doctor', 'patient', 'health', 'measurement', 'is', 'are', 'the', 'and', 'or', 'hello', 'hi', 'about', 'system', 'help', 'can', 'you']
    english_words = sum(1 for word in english_keywords if word in message_lower)
    
    # Smart detection: Vietnamese chars = definite Vietnamese
    if vietnamese_chars:
        response_language = "vi"
    # No Vietnamese chars: compare word counts
    elif vietnamese_words > english_words:
        response_language = "vi"
    elif english_words > vietnamese_words:
        response_language = "en" 
    # Equal or no keywords: check common patterns
    else:
        # Common English greetings and short phrases
        english_patterns = ['hello', 'hi', 'hey', 'good morning', 'good afternoon', 'how are you', 'thank you', 'thanks']
        if any(pattern in message_lower for pattern in english_patterns):
            response_language = "en"
        else:
            response_language = "vi"  # Default to Vietnamese
        
    # Override: If user asks about Vietnamese terms, always respond in Vietnamese
    vietnamese_terms = ['huy·∫øt √°p', 's·ª©c kh·ªèe', 'b√°c sƒ©', 'b·ªánh nh√¢n', 'c√°ch s·ª≠ d·ª•ng', 'h∆∞·ªõng d·∫´n']
    if any(term in message_lower for term in vietnamese_terms):
        response_language = "vi"
    
    # Blood pressure specific responses
    if any(word in message_lower for word in ["huy·∫øt √°p", "blood pressure", "bp", "ƒëo"]):
        if response_language == "vi":
            if role == "PATIENT":
                return f"Ch√†o {user_name}! D·ª±a tr√™n th√¥ng tin y t·∫ø:\n\n{medical_context}\n\nT√¥i khuy√™n b·∫°n n√™n:\n- ƒêo huy·∫øt √°p ƒë·ªÅu ƒë·∫∑n c√πng gi·ªù m·ªói ng√†y\n- Ghi l·∫°i k·∫øt qu·∫£ ƒë·ªÉ theo d√µi xu h∆∞·ªõng\n- Tu√¢n th·ªß ch·∫ø ƒë·ªô ƒÉn DASH\n- T·∫≠p th·ªÉ d·ª•c ƒë·ªÅu ƒë·∫∑n\n\nN·∫øu huy·∫øt √°p >180/120 mmHg, h√£y ƒë·∫øn b·ªánh vi·ªán ngay l·∫≠p t·ª©c."
            else:  # DOCTOR/ADMIN
                return f"Th√¥ng tin l√¢m s√†ng cho b√°c sƒ© {user_name}:\n\n{medical_context}\n\nKhuy·∫øn ngh·ªã:\n- ƒê√°nh gi√° nguy c∆° tim m·∫°ch to√†n di·ªán\n- Xem x√©t ƒëi·ªÅu ch·ªânh thu·ªëc n·∫øu c·∫ßn\n- Gi√°o d·ª•c b·ªánh nh√¢n v·ªÅ l·ªëi s·ªëng\n- Theo d√µi tu√¢n th·ªß ƒëi·ªÅu tr·ªã"
        else:  # English
            if role == "PATIENT":
                return f"Hello {user_name}! Based on medical information:\n\n{medical_context}\n\nI recommend:\n- Monitor BP regularly at same time daily\n- Record results to track trends\n- Follow DASH diet\n- Exercise regularly\n\nIf BP >180/120 mmHg, seek emergency care immediately."
            else:
                return f"Clinical information for Dr. {user_name}:\n\n{medical_context}\n\nRecommendations:\n- Assess comprehensive cardiovascular risk\n- Consider medication adjustment if needed\n- Patient lifestyle education\n- Monitor treatment adherence"
    
    # Diet and nutrition responses
    elif any(word in message_lower for word in ["ƒÉn", "diet", "nutrition", "dinh d∆∞·ª°ng"]):
        if response_language == "vi":
            return f"Ch·∫ø ƒë·ªô dinh d∆∞·ª°ng cho {user_name}:\n\n{medical_context}\n\nL·ªùi khuy√™n th·ª±c t·∫ø:\n- N·∫•u ƒÉn t·∫°i nh√† ƒë·ªÉ ki·ªÉm so√°t mu·ªëi\n- ƒê·ªçc nh√£n th·ª±c ph·∫©m\n- Thay th·∫ø mu·ªëi b·∫±ng th·∫£o m·ªôc, gia v·ªã\n- ƒÇn nhi·ªÅu b·ªØa nh·ªè trong ng√†y"
        else:
            return f"Nutrition guidance for {user_name}:\n\n{medical_context}\n\nPractical tips:\n- Cook at home to control sodium\n- Read food labels\n- Use herbs and spices instead of salt\n- Eat smaller, frequent meals"
    
    # Exercise responses  
    elif any(word in message_lower for word in ["t·∫≠p", "exercise", "v·∫≠n ƒë·ªông"]):
        if response_language == "vi":
            return f"H∆∞·ªõng d·∫´n t·∫≠p luy·ªán cho {user_name}:\n\n{medical_context}\n\nB·∫Øt ƒë·∫ßu t·ª´ t·ª´:\n- Tu·∫ßn 1-2: ƒëi b·ªô 15-20 ph√∫t/ng√†y\n- Tu·∫ßn 3-4: tƒÉng l√™n 30 ph√∫t\n- Lu√¥n ƒëo huy·∫øt √°p tr∆∞·ªõc v√† sau t·∫≠p"
        else:
            return f"Exercise guidance for {user_name}:\n\n{medical_context}\n\nStart gradually:\n- Week 1-2: walk 15-20 minutes daily\n- Week 3-4: increase to 30 minutes\n- Always check BP before and after exercise"
    
    # General health responses
    else:
        if response_language == "vi":
            return f"Xin ch√†o {user_name}! T√¥i l√† tr·ª£ l√Ω s·ª©c kh·ªèe SmartBP. {medical_context}\n\nT√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªÅ:\n- Qu·∫£n l√Ω huy·∫øt √°p\n- Ch·∫ø ƒë·ªô ƒÉn u·ªëng\n- T·∫≠p th·ªÉ d·ª•c\n- Thu·ªëc v√† ƒëi·ªÅu tr·ªã\n\nB·∫°n c√≥ c√¢u h·ªèi g√¨ v·ªÅ s·ª©c kh·ªèe kh√¥ng?"
        else:
            return f"Hello {user_name}! I'm your SmartBP health assistant. {medical_context}\n\nI can help you with:\n- Blood pressure management\n- Diet and nutrition\n- Exercise guidance\n- Medications and treatment\n\nWhat health questions do you have?"

def generate_mock_response(message: str, role: str, language: str = "vi") -> str:
    """Generate mock responses when RAG system is not available"""
    message_lower = message.lower()
    
    # Blood pressure related responses
    bp_keywords = ["huy·∫øt √°p", "blood pressure", "bp", "ƒëo", "measurement", "mmhg"]
    diet_keywords = ["ƒÉn", "th·ª©c ƒÉn", "diet", "food", "nutrition", "dinh d∆∞·ª°ng"]
    exercise_keywords = ["t·∫≠p th·ªÉ d·ª•c", "exercise", "workout", "th·ªÉ d·ª•c", "v·∫≠n ƒë·ªông"]
    
    if language == "vi":
        if any(keyword in message_lower for keyword in bp_keywords):
            if role == "PATIENT":
                return "T√¥i hi·ªÉu b·∫°n quan t√¢m v·ªÅ huy·∫øt √°p. ƒê·ªÉ ƒëo huy·∫øt √°p ch√≠nh x√°c, b·∫°n n√™n ng·ªìi th·∫≥ng, th∆∞ gi√£n 5 ph√∫t tr∆∞·ªõc khi ƒëo. Huy·∫øt √°p b√¨nh th∆∞·ªùng d∆∞·ªõi 120/80 mmHg. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng ƒëo huy·∫øt √°p trong ·ª©ng d·ª•ng SmartBP ƒë·ªÉ theo d√µi."
            elif role == "DOCTOR":
                return "V·ªÅ qu·∫£n l√Ω huy·∫øt √°p b·ªánh nh√¢n, t√¥i khuy·∫øn ngh·ªã theo d√µi th∆∞·ªùng xuy√™n v√† ph√¢n t√≠ch xu h∆∞·ªõng. B·∫°n c√≥ th·ªÉ xem d·ªØ li·ªáu ƒëo t·ª´ h·ªá th·ªëng SmartBP ƒë·ªÉ ƒë√°nh gi√° t√¨nh tr·∫°ng b·ªánh nh√¢n."
        elif any(keyword in message_lower for keyword in diet_keywords):
            return "Ch·∫ø ƒë·ªô ƒÉn DASH (Dietary Approaches to Stop Hypertension) ƒë∆∞·ª£c khuy·∫øn ngh·ªã cho ng∆∞·ªùi c√≥ huy·∫øt √°p cao: TƒÉng rau c·ªß, hoa qu·∫£, gi·∫£m mu·ªëi, h·∫°n ch·∫ø th·ª©c ƒÉn ch·∫ø bi·∫øn s·∫µn."
        elif any(keyword in message_lower for keyword in exercise_keywords):
            return "T·∫≠p th·ªÉ d·ª•c ƒë·ªÅu ƒë·∫∑n gi√∫p gi·∫£m huy·∫øt √°p hi·ªáu qu·∫£. N√™n t·∫≠p √≠t nh·∫•t 30 ph√∫t/ng√†y, 5 ng√†y/tu·∫ßn. C√°c b√†i t·∫≠p tim m·∫°ch nh·∫π nh∆∞ ƒëi b·ªô nhanh, b∆°i l·ªôi r·∫•t t·ªët."
        else:
            return "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI c·ªßa SmartBP. Hi·ªán t·∫°i h·ªá th·ªëng ƒëang trong ch·∫ø ƒë·ªô c∆° b·∫£n. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ huy·∫øt √°p, ch·∫ø ƒë·ªô ƒÉn u·ªëng, ho·∫∑c t·∫≠p th·ªÉ d·ª•c. T√¥i s·∫Ω c·ªë g·∫Øng h·ªó tr·ª£ b·∫°n t·ªët nh·∫•t c√≥ th·ªÉ."
    else:  # English
        if any(keyword in message_lower for keyword in bp_keywords):
            if role == "PATIENT":
                return "I understand you're asking about blood pressure. For accurate measurement, sit upright and relax for 5 minutes before measuring. Normal blood pressure is below 120/80 mmHg. You can use the SmartBP app's measurement feature to track your readings."
            elif role == "DOCTOR":
                return "For patient blood pressure management, I recommend regular monitoring and trend analysis. You can review measurement data from the SmartBP system to assess patient status."
        elif any(keyword in message_lower for keyword in diet_keywords):
            return "The DASH diet (Dietary Approaches to Stop Hypertension) is recommended for high blood pressure: Increase vegetables, fruits, reduce salt, limit processed foods."
        elif any(keyword in message_lower for keyword in exercise_keywords):
            return "Regular exercise effectively helps reduce blood pressure. Aim for at least 30 minutes/day, 5 days/week. Light cardiovascular exercises like brisk walking and swimming are excellent."
        else:
            return "Hello! I'm the SmartBP AI assistant. The system is currently in basic mode. You can ask about blood pressure, diet, or exercise. I'll do my best to help you."

def initialize_llm():
    """Initialize Ollama LLM"""
    try:
        logging.info("üîµ Testing Ollama connection...")
        ollama_client = ollama.Client()
        ollama_client.list()  # Test connection
        logging.info("üîµ Creating ChatOllama instance...")
        llm = ChatOllama(model=MODEL_NAME, temperature=0.7, base_url=OLLAMA_BASE_URL)
        logging.info(f"‚úÖ Using Ollama LLM: {MODEL_NAME}")
        return llm
    except Exception as e:
        logging.error(f"‚ùå Ollama LLM failed: {e}")
        logging.warning("‚ö†Ô∏è Make sure Ollama is running: ollama serve")
        return None

def analyze_bp_risk(measurements: List[MeasurementData]) -> str:
    """Analyze blood pressure risk based on recent measurements"""
    if not measurements:
        return "Unknown"
    
    recent = measurements[:5]  # Last 5 measurements
    avg_sys = sum(m.sys for m in recent) / len(recent)
    avg_dia = sum(m.dia for m in recent) / len(recent)
    
    if avg_sys >= 180 or avg_dia >= 110:
        return "Critical - Hypertensive Crisis"
    elif avg_sys >= 140 or avg_dia >= 90:
        return "High - Stage 2 Hypertension"
    elif avg_sys >= 130 or avg_dia >= 80:
        return "Elevated - Stage 1 Hypertension"
    elif avg_sys >= 120:
        return "Elevated - Prehypertension"
    else:
        return "Normal"

def format_patient_context(context: ChatContext) -> Dict[str, str]:
    """Format patient context for prompt template"""
    user = context.user
    patient_data = context.role_specific_data or {}
    
    # Calculate age if date of birth provided
    age = "Unknown"
    if user.dateOfBirth:
        try:
            birth_date = datetime.fromisoformat(user.dateOfBirth.replace('Z', '+00:00'))
            age = str(datetime.now().year - birth_date.year)
        except:
            pass
    
    # Format measurements
    measurements = patient_data.get('latest_measurements', [])
    recent_measurements = ""
    if measurements:
        recent_measurements = f"Recent: {measurements[0]['sys']}/{measurements[0]['dia']} mmHg"
    
    return {
        "patient_name": user.name or "Patient",
        "patient_age": age,
        "avg_bp": f"{patient_data.get('avg_sys', 0):.0f}/{patient_data.get('avg_dia', 0):.0f}",
        "measurement_count": str(patient_data.get('measurement_count', 0)),
        "risk_level": patient_data.get('risk_assessment', 'Unknown'),
        "recent_measurements": recent_measurements
    }

def create_conversation_chain(role: str, context: ChatContext, language: str = "vi"):
    """Create a conversation chain for specific role with context and language"""
    global vectorstore
    
    # If vectorstore is None, try to initialize RAG system
    if not vectorstore:
        logging.warning("‚ö†Ô∏è Vectorstore not initialized, attempting to initialize RAG system...")
        try:
            initialize_rag_system()
        except Exception as e:
            logging.error(f"‚ùå Failed to initialize RAG system: {e}")
            # Return None to trigger fallback responses
            return None
    
    # If still no vectorstore after initialization attempt, return None for fallback
    if not vectorstore:
        logging.warning("‚ö†Ô∏è No vectorstore available, will use fallback responses")
        return None
    
    # Use unified template that adapts to user's language
    # Format context based on role for unified template
    if role == "PATIENT":
        template_vars = format_patient_context(context)
    elif role == "DOCTOR":
        doctor_data = context.role_specific_data or {}
        template_vars = {
            "patient_name": context.user.name or "Doctor",
            "patient_age": "N/A",
            "avg_bp": "Various patients",
            "measurement_count": doctor_data.get('assigned_patients_count', 0),
            "risk_level": f"{doctor_data.get('pending_reviews', 0)} pending reviews",
            "recent_measurements": ", ".join(doctor_data.get('recent_alerts', []) or ["No recent alerts"])
        }
    else:  # ADMIN
        template_vars = {
            "patient_name": context.user.name or "Admin",
            "patient_age": "N/A",
            "avg_bp": "System wide",
            "measurement_count": "All users",
            "risk_level": "System monitoring",
            "recent_measurements": "System logs available"
        }
    
    # Create unified prompt template
    final_template = UNIFIED_TEMPLATE.format(
        patient_name=template_vars.get('patient_name', 'User'),
        patient_age=template_vars.get('patient_age', 'Unknown'),
        role=role,
        avg_bp=template_vars.get('avg_bp', 'Unknown'),
        measurement_count=template_vars.get('measurement_count', '0'),
        risk_level=template_vars.get('risk_level', 'Unknown'),
        recent_measurements=template_vars.get('recent_measurements', 'None'),
        chat_history="{chat_history}",
        context="{context}",
        question="{question}"
    )
    
    prompt = PromptTemplate(
        template=final_template,
        input_variables=["chat_history", "context", "question"]
    )
    
    # Initialize LLM with fallbacks
    llm = initialize_llm()
    
    # Return None if no LLM is available (will trigger mock responses)
    if llm is None or vectorstore is None:
        logging.warning("‚ö†Ô∏è No LLM or vectorstore available - conversation chain disabled")
        return None
    
    # Create a simple retriever + LLM chain (replacing deprecated ConversationalRetrievalChain)
    # Return a dict-like object that mimics the chain interface
    class SimpleRAGChain:
        def __init__(self, llm, retriever, prompt):
            self.llm = llm
            self.retriever = retriever
            self.prompt = prompt
            self.chat_history = []
        
        def invoke(self, inputs):
            """Simplified chain invocation"""
            try:
                question = inputs.get("question", "")
                chat_hist = inputs.get("chat_history", "")
                
                # Retrieve context
                docs = self.retriever.invoke(question)
                doc_text = "\n".join([doc.page_content for doc in docs])
                
                # Format prompt with retrieved docs and chat history
                formatted_prompt = self.prompt.format(
                    chat_history=chat_hist,
                    context=doc_text,
                    question=question
                )
                
                # Get LLM response
                response = self.llm.invoke(formatted_prompt)
                if hasattr(response, 'content'):
                    return {"text": response.content}
                return {"text": str(response)}
            except Exception as e:
                logging.error(f"Chain invocation error: {e}")
                return {"text": ""}
    
    chain = SimpleRAGChain(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        prompt=prompt
    )
    
    return chain

# No longer using @app.on_event - using lifespan context manager instead
# Vectorstore is now initialized in the lifespan handler
@app.post("/chat")
async def enhanced_chat_endpoint(request: EnhancedChatRequest):
    """Enhanced chat endpoint with full SBM integration and multilingual support"""
    try:
        logging.info(f"üîµ Received chat request from user: {request.user_id}")
        logging.info(f"üîµ Message: {request.message}")
        logging.info(f"üîµ Language: {request.language}")
        logging.info(f"üîµ Context user role: {request.context.user.role}")
        
        # Validate request
        if not request.message.strip():
            logging.error("‚ùå Empty message")
            raise HTTPException(status_code=400, detail="Message cannot be empty")
        
        logging.info("üîµ Starting language detection...")
        # Determine language to use
        if request.language == "auto":
            # Auto-detect language from message
            detected_lang = detect_language(request.message)
        else:
            # Use specified language
            detected_lang = request.language or "vi"
        
        logging.info(f"üîµ Detected language: {detected_lang}")
        
        # Generate conversation ID if not provided
        conv_id = request.conversation_id or f"{request.user_id}_{int(time.time())}"
        logging.info(f"üîµ Conversation ID: {conv_id}")
        
        # Create conversation chain key with language
        chain_key = f"{conv_id}_{detected_lang}"
        logging.info(f"üîµ Chain key: {chain_key}")
        
        # Create or get conversation chain
        if chain_key not in conversation_chains:
            logging.info("üîµ Creating new conversation chain...")
            conversation_chains[chain_key] = create_conversation_chain(
                request.context.user.role, 
                request.context,
                detected_lang
            )
        else:
            logging.info("üîµ Using existing conversation chain...")
        
        chain = conversation_chains[chain_key]
        logging.info(f"üîµ Chain status: {chain is not None}")
        
        # Process the question
        if chain is None or vectorstore is None:
            logging.info("üîµ Using fallback response (no chain/vectorstore)")
            # Use enhanced fallback response when RAG system is not available
            medical_context = get_fallback_medical_context(request.message)
            response_text = generate_enhanced_mock_response(
                request.message, 
                request.context.user.role, 
                detected_lang,
                medical_context,
                request.context
            )
        else:
            try:
                logging.info("üîµ Executing chain query...")
                result = chain.invoke({"question": request.message, "chat_history": ""})
                response_text = result["text"]
                logging.info("üîµ Chain execution successful")
            except Exception as e:
                logging.error(f"‚ùå Chain execution failed: {e}")
                import traceback
                logging.error(f"Full traceback: {traceback.format_exc()}")
                # Enhanced fallback on error
                medical_context = get_fallback_medical_context(request.message)
                response_text = generate_enhanced_mock_response(
                    request.message, 
                    request.context.user.role, 
                    detected_lang,
                    medical_context,
                    request.context
                )
        
        logging.info("üîµ Analyzing response urgency...")
        # Analyze response for medical urgency (multilingual keywords)
        urgent_keywords = ["kh·∫©n c·∫•p", "ngay l·∫≠p t·ª©c", "emergency", "immediately", "crisis", "urgent"]
        requires_attention = any(keyword in response_text.lower() for keyword in urgent_keywords)
        
        logging.info("üîµ Generating suggestions...")
        # Generate suggestions based on role and language
        suggestions = []
        if request.context.user.role == "PATIENT":
            if detected_lang == "vi":
                suggestions = [
                    "Xem l·ªãch s·ª≠ ƒëo huy·∫øt √°p",
                    "H∆∞·ªõng d·∫´n ƒëo huy·∫øt √°p ƒë√∫ng c√°ch", 
                    "T∆∞ v·∫•n ch·∫ø ƒë·ªô ƒÉn u·ªëng"
                ]
            else:
                suggestions = [
                    "View blood pressure history",
                    "Blood pressure measurement guide",
                    "Diet and nutrition advice"
                ]
        elif request.context.user.role == "DOCTOR":
            if detected_lang == "vi":
                suggestions = [
                    "Xem b·ªánh nh√¢n c·∫ßn theo d√µi",
                    "Ph√¢n t√≠ch xu h∆∞·ªõng huy·∫øt √°p",
                    "T·∫°o ghi ch√∫ kh√°m b·ªánh"
                ]
            else:
                suggestions = [
                    "View patients requiring monitoring",
                    "Analyze blood pressure trends",
                    "Create clinical notes"
                ]
        
        logging.info("üîµ Creating response object...")
        # Create response
        response = EnhancedChatResponse(
            success=True,
            response=response_text,
            conversation_id=conv_id,
            suggestions=suggestions,
            requires_medical_attention=requires_attention,
            detected_language=detected_lang,
            data_insights=DataInsights(
                mentioned_measurements="huy·∫øt √°p" in request.message.lower() or "blood pressure" in request.message.lower(),
                health_recommendations=[],
                follow_up_actions=[]
            )
        )
        
        logging.info("‚úÖ Chat request completed successfully")
        return response
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logging.error(f"Chat endpoint error: {e}")
        logging.error(f"Full traceback: {error_details}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 5001))  # Use PORT from .env or default to 5001 to avoid conflicts
    uvicorn.run(app, host="0.0.0.0", port=port)

